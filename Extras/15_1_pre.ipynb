{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "import ast  \n",
    "import random\n",
    "import os\n",
    "os.chdir('Resources/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('1_CC_Combined_Data.csv')\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "X = df.drop(columns=[\"HeartDisease\"])\n",
    "Y = LabelEncoder().fit_transform(df[\"HeartDisease\"])\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "for col in X.select_dtypes(include=['object']).columns:\n",
    "    X[col] = label_encoder.fit_transform(X[col])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.8375634517766497 0.8375634517766497 1\n",
      "2 0.8875 0.8875 2\n",
      "3 0.883495145631068 0.8875 2\n",
      "4 0.9162011173184358 0.9162011173184358 4\n",
      "5 0.9019607843137255 0.9162011173184358 4\n",
      "6 0.9251336898395722 0.9251336898395722 6\n",
      "7 0.92 0.9251336898395722 6\n",
      "8 0.9206349206349206 0.9251336898395722 6\n",
      "9 0.9207920792079208 0.9251336898395722 6\n",
      "10 0.9206349206349206 0.9251336898395722 6\n",
      "11 0.9019607843137255 0.9251336898395722 6\n",
      "12 0.9128205128205128 0.9251336898395722 6\n",
      "13 0.9108910891089109 0.9251336898395722 6\n",
      "14 0.9179487179487179 0.9251336898395722 6\n",
      "15 0.9108910891089109 0.9251336898395722 6\n",
      "16 0.9175257731958762 0.9251336898395722 6\n",
      "17 0.9154228855721394 0.9251336898395722 6\n",
      "18 0.9183673469387755 0.9251336898395722 6\n",
      "19 0.9108910891089109 0.9251336898395722 6\n",
      "20 0.9141414141414141 0.9251336898395722 6\n",
      "21 0.9117647058823529 0.9251336898395722 6\n",
      "22 0.9195979899497487 0.9251336898395722 6\n",
      "23 0.9117647058823529 0.9251336898395722 6\n",
      "24 0.9158415841584159 0.9251336898395722 6\n",
      "25 0.9033816425120773 0.9251336898395722 6\n",
      "26 0.915 0.9251336898395722 6\n",
      "27 0.9130434782608695 0.9251336898395722 6\n",
      "28 0.9117647058823529 0.9251336898395722 6\n",
      "29 0.912621359223301 0.9251336898395722 6\n",
      "30 0.9207920792079208 0.9251336898395722 6\n",
      "31 0.9170731707317074 0.9251336898395722 6\n",
      "32 0.9203980099502488 0.9251336898395722 6\n",
      "33 0.9203980099502488 0.9251336898395722 6\n",
      "34 0.9246231155778895 0.9251336898395722 6\n",
      "35 0.925 0.9251336898395722 6\n",
      "36 0.9292929292929293 0.9292929292929293 36\n",
      "37 0.9178743961352657 0.9292929292929293 36\n",
      "38 0.9261083743842364 0.9292929292929293 36\n",
      "39 0.9271844660194175 0.9292929292929293 36\n",
      "40 0.9253731343283582 0.9292929292929293 36\n",
      "41 0.9223300970873787 0.9292929292929293 36\n",
      "42 0.93 0.93 42\n",
      "43 0.9261083743842364 0.93 42\n",
      "44 0.9261083743842364 0.93 42\n",
      "45 0.9170731707317074 0.93 42\n",
      "46 0.9303482587064676 0.9303482587064676 46\n",
      "47 0.9166666666666666 0.9303482587064676 46\n",
      "48 0.9211822660098522 0.9303482587064676 46\n",
      "49 0.9215686274509803 0.9303482587064676 46\n",
      "50 0.9303482587064676 0.9303482587064676 46\n",
      "51 0.9211822660098522 0.9303482587064676 46\n",
      "52 0.9257425742574258 0.9303482587064676 46\n",
      "53 0.9264705882352942 0.9303482587064676 46\n",
      "54 0.9257425742574258 0.9303482587064676 46\n",
      "55 0.926829268292683 0.9303482587064676 46\n",
      "56 0.9313725490196079 0.9313725490196079 56\n",
      "57 0.9230769230769231 0.9313725490196079 56\n",
      "58 0.9320388349514563 0.9320388349514563 58\n",
      "59 0.9186602870813397 0.9320388349514563 58\n",
      "60 0.927536231884058 0.9320388349514563 58\n",
      "61 0.919047619047619 0.9320388349514563 58\n",
      "62 0.9320388349514563 0.9320388349514563 58\n",
      "63 0.9278846153846154 0.9320388349514563 58\n",
      "64 0.927536231884058 0.9320388349514563 58\n",
      "65 0.9230769230769231 0.9320388349514563 58\n",
      "66 0.9230769230769231 0.9320388349514563 58\n",
      "67 0.9234449760765551 0.9320388349514563 58\n",
      "68 0.9234449760765551 0.9320388349514563 58\n",
      "69 0.9234449760765551 0.9320388349514563 58\n",
      "70 0.9278846153846154 0.9320388349514563 58\n",
      "71 0.9278846153846154 0.9320388349514563 58\n",
      "72 0.9368932038834952 0.9368932038834952 72\n",
      "73 0.9368932038834952 0.9368932038834952 72\n",
      "74 0.9365853658536586 0.9368932038834952 72\n",
      "75 0.9368932038834952 0.9368932038834952 72\n",
      "76 0.9368932038834952 0.9368932038834952 72\n",
      "77 0.9278846153846154 0.9368932038834952 72\n",
      "78 0.9323671497584541 0.9368932038834952 72\n",
      "79 0.9234449760765551 0.9368932038834952 72\n",
      "80 0.9234449760765551 0.9368932038834952 72\n",
      "81 0.9234449760765551 0.9368932038834952 72\n",
      "82 0.9320388349514563 0.9368932038834952 72\n",
      "83 0.9278846153846154 0.9368932038834952 72\n",
      "84 0.9368932038834952 0.9368932038834952 72\n",
      "85 0.9323671497584541 0.9368932038834952 72\n",
      "86 0.9368932038834952 0.9368932038834952 72\n",
      "87 0.9323671497584541 0.9368932038834952 72\n",
      "88 0.9411764705882353 0.9411764705882353 88\n",
      "89 0.9365853658536586 0.9411764705882353 88\n",
      "90 0.9408866995073891 0.9411764705882353 88\n",
      "91 0.9411764705882353 0.9411764705882353 88\n",
      "92 0.9408866995073891 0.9411764705882353 88\n",
      "93 0.9411764705882353 0.9411764705882353 88\n",
      "94 0.9408866995073891 0.9411764705882353 88\n",
      "95 0.9414634146341463 0.9414634146341463 95\n",
      "96 0.9408866995073891 0.9414634146341463 95\n",
      "97 0.9411764705882353 0.9414634146341463 95\n",
      "98 0.9408866995073891 0.9414634146341463 95\n",
      "99 0.9365853658536586 0.9414634146341463 95\n",
      "100 0.9356435643564357 0.9414634146341463 95\n",
      "101 0.9323671497584541 0.9414634146341463 95\n",
      "102 0.9317073170731708 0.9414634146341463 95\n",
      "103 0.9320388349514563 0.9414634146341463 95\n",
      "104 0.9313725490196079 0.9414634146341463 95\n",
      "105 0.9317073170731708 0.9414634146341463 95\n",
      "106 0.9359605911330049 0.9414634146341463 95\n",
      "107 0.9362745098039216 0.9414634146341463 95\n",
      "108 0.9359605911330049 0.9414634146341463 95\n",
      "109 0.9362745098039216 0.9414634146341463 95\n",
      "110 0.9359605911330049 0.9414634146341463 95\n",
      "111 0.926829268292683 0.9414634146341463 95\n",
      "112 0.9310344827586207 0.9414634146341463 95\n",
      "113 0.9310344827586207 0.9414634146341463 95\n",
      "114 0.9356435643564357 0.9414634146341463 95\n",
      "115 0.9310344827586207 0.9414634146341463 95\n",
      "116 0.9310344827586207 0.9414634146341463 95\n",
      "117 0.9313725490196079 0.9414634146341463 95\n",
      "118 0.9310344827586207 0.9414634146341463 95\n",
      "119 0.9310344827586207 0.9414634146341463 95\n",
      "120 0.9310344827586207 0.9414634146341463 95\n",
      "121 0.9264705882352942 0.9414634146341463 95\n",
      "122 0.9310344827586207 0.9414634146341463 95\n",
      "123 0.9313725490196079 0.9414634146341463 95\n",
      "124 0.9356435643564357 0.9414634146341463 95\n",
      "125 0.9356435643564357 0.9414634146341463 95\n",
      "126 0.9356435643564357 0.9414634146341463 95\n",
      "127 0.9313725490196079 0.9414634146341463 95\n",
      "128 0.9313725490196079 0.9414634146341463 95\n",
      "129 0.9313725490196079 0.9414634146341463 95\n",
      "130 0.9313725490196079 0.9414634146341463 95\n",
      "131 0.9313725490196079 0.9414634146341463 95\n",
      "132 0.9313725490196079 0.9414634146341463 95\n",
      "133 0.9313725490196079 0.9414634146341463 95\n",
      "134 0.9313725490196079 0.9414634146341463 95\n",
      "135 0.9317073170731708 0.9414634146341463 95\n",
      "136 0.9317073170731708 0.9414634146341463 95\n",
      "137 0.9227053140096618 0.9414634146341463 95\n",
      "138 0.9317073170731708 0.9414634146341463 95\n",
      "139 0.9271844660194175 0.9414634146341463 95\n",
      "140 0.9317073170731708 0.9414634146341463 95\n",
      "141 0.9317073170731708 0.9414634146341463 95\n",
      "142 0.9317073170731708 0.9414634146341463 95\n",
      "143 0.9317073170731708 0.9414634146341463 95\n",
      "144 0.9317073170731708 0.9414634146341463 95\n",
      "145 0.9271844660194175 0.9414634146341463 95\n",
      "146 0.9271844660194175 0.9414634146341463 95\n",
      "147 0.9271844660194175 0.9414634146341463 95\n",
      "148 0.9271844660194175 0.9414634146341463 95\n",
      "149 0.9227053140096618 0.9414634146341463 95\n",
      "150 0.9227053140096618 0.9414634146341463 95\n",
      "151 0.9227053140096618 0.9414634146341463 95\n",
      "152 0.9227053140096618 0.9414634146341463 95\n",
      "153 0.9227053140096618 0.9414634146341463 95\n",
      "154 0.9223300970873787 0.9414634146341463 95\n",
      "155 0.9227053140096618 0.9414634146341463 95\n",
      "156 0.9223300970873787 0.9414634146341463 95\n",
      "157 0.9227053140096618 0.9414634146341463 95\n",
      "158 0.9227053140096618 0.9414634146341463 95\n",
      "159 0.9227053140096618 0.9414634146341463 95\n",
      "160 0.9227053140096618 0.9414634146341463 95\n",
      "161 0.9227053140096618 0.9414634146341463 95\n",
      "162 0.9223300970873787 0.9414634146341463 95\n",
      "163 0.9227053140096618 0.9414634146341463 95\n",
      "164 0.9223300970873787 0.9414634146341463 95\n",
      "165 0.9223300970873787 0.9414634146341463 95\n",
      "166 0.9223300970873787 0.9414634146341463 95\n",
      "167 0.9223300970873787 0.9414634146341463 95\n",
      "168 0.9223300970873787 0.9414634146341463 95\n",
      "169 0.9227053140096618 0.9414634146341463 95\n",
      "170 0.9223300970873787 0.9414634146341463 95\n",
      "171 0.9223300970873787 0.9414634146341463 95\n",
      "172 0.9223300970873787 0.9414634146341463 95\n",
      "173 0.9227053140096618 0.9414634146341463 95\n",
      "174 0.9227053140096618 0.9414634146341463 95\n",
      "175 0.9227053140096618 0.9414634146341463 95\n",
      "176 0.9227053140096618 0.9414634146341463 95\n",
      "177 0.9227053140096618 0.9414634146341463 95\n",
      "178 0.9227053140096618 0.9414634146341463 95\n",
      "179 0.9227053140096618 0.9414634146341463 95\n",
      "180 0.9227053140096618 0.9414634146341463 95\n",
      "181 0.9227053140096618 0.9414634146341463 95\n",
      "182 0.9227053140096618 0.9414634146341463 95\n",
      "183 0.9227053140096618 0.9414634146341463 95\n",
      "184 0.9227053140096618 0.9414634146341463 95\n",
      "185 0.9227053140096618 0.9414634146341463 95\n",
      "186 0.9227053140096618 0.9414634146341463 95\n",
      "187 0.9227053140096618 0.9414634146341463 95\n",
      "188 0.9223300970873787 0.9414634146341463 95\n",
      "189 0.9182692307692307 0.9414634146341463 95\n",
      "190 0.9178743961352657 0.9414634146341463 95\n",
      "191 0.919047619047619 0.9414634146341463 95\n",
      "192 0.9186602870813397 0.9414634146341463 95\n",
      "193 0.9186602870813397 0.9414634146341463 95\n",
      "194 0.9186602870813397 0.9414634146341463 95\n",
      "195 0.919047619047619 0.9414634146341463 95\n",
      "196 0.919047619047619 0.9414634146341463 95\n",
      "197 0.919047619047619 0.9414634146341463 95\n",
      "198 0.9186602870813397 0.9414634146341463 95\n",
      "199 0.9186602870813397 0.9414634146341463 95\n"
     ]
    }
   ],
   "source": [
    "# 1.1 - RF (F1)\n",
    "#------------------\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score, precision_score\n",
    "\n",
    "mx=0\n",
    "ind=0\n",
    "#3637 0.861904761904762 0.9323671497584541 2046\n",
    "\n",
    "for i in range(1, 200):\n",
    "    seed = i\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, Y, test_size=0.2, random_state=2046)\n",
    "\n",
    "    rf = RandomForestClassifier(random_state=1458, n_estimators=seed)\n",
    "    rf.fit(X_train, y_train)\n",
    "    rf_pred = rf.predict(X_test)\n",
    "\n",
    "    combined_pre = precision_score(y_test, rf_pred)\n",
    "\n",
    "    if combined_pre>mx:\n",
    "        mx=combined_pre\n",
    "        ind=i\n",
    "\n",
    "    print(i, combined_pre, mx, ind)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
