{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "import ast  \n",
    "import random\n",
    "import os\n",
    "os.chdir('Resources/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('1_CC_Structured_Data.csv')\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "X = df.drop(columns=[\"HeartDisease\"])\n",
    "Y = LabelEncoder().fit_transform(df[\"HeartDisease\"])\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "for col in X.select_dtypes(include=['object']).columns:\n",
    "    X[col] = label_encoder.fit_transform(X[col])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.8164383561643835 0.8164383561643835 1\n",
      "2 0.810958904109589 0.8164383561643835 1\n",
      "3 0.8410958904109589 0.8410958904109589 3\n",
      "4 0.8438356164383561 0.8438356164383561 4\n",
      "5 0.8465753424657534 0.8465753424657534 5\n",
      "6 0.8684931506849315 0.8684931506849315 6\n",
      "7 0.8657534246575342 0.8684931506849315 6\n",
      "8 0.8767123287671232 0.8767123287671232 8\n",
      "9 0.873972602739726 0.8767123287671232 8\n",
      "10 0.8821917808219178 0.8821917808219178 10\n",
      "11 0.8767123287671232 0.8821917808219178 10\n",
      "12 0.8904109589041096 0.8904109589041096 12\n",
      "13 0.8712328767123287 0.8904109589041096 12\n",
      "14 0.8931506849315068 0.8931506849315068 14\n",
      "15 0.8876712328767123 0.8931506849315068 14\n",
      "16 0.8958904109589041 0.8958904109589041 16\n",
      "17 0.8876712328767123 0.8958904109589041 16\n",
      "18 0.8958904109589041 0.8958904109589041 16\n",
      "19 0.8821917808219178 0.8958904109589041 16\n",
      "20 0.8958904109589041 0.8958904109589041 16\n",
      "21 0.8931506849315068 0.8958904109589041 16\n",
      "22 0.8931506849315068 0.8958904109589041 16\n",
      "23 0.8904109589041096 0.8958904109589041 16\n",
      "24 0.8986301369863013 0.8986301369863013 24\n",
      "25 0.8986301369863013 0.8986301369863013 24\n",
      "26 0.8986301369863013 0.8986301369863013 24\n",
      "27 0.9013698630136986 0.9013698630136986 27\n",
      "28 0.8986301369863013 0.9013698630136986 27\n",
      "29 0.9013698630136986 0.9013698630136986 27\n",
      "30 0.8986301369863013 0.9013698630136986 27\n",
      "31 0.8931506849315068 0.9013698630136986 27\n",
      "32 0.8931506849315068 0.9013698630136986 27\n",
      "33 0.8958904109589041 0.9013698630136986 27\n",
      "34 0.8958904109589041 0.9013698630136986 27\n",
      "35 0.8958904109589041 0.9013698630136986 27\n",
      "36 0.8986301369863013 0.9013698630136986 27\n",
      "37 0.8958904109589041 0.9013698630136986 27\n",
      "38 0.8931506849315068 0.9013698630136986 27\n",
      "39 0.8931506849315068 0.9013698630136986 27\n",
      "40 0.8931506849315068 0.9013698630136986 27\n",
      "41 0.8931506849315068 0.9013698630136986 27\n",
      "42 0.8904109589041096 0.9013698630136986 27\n",
      "43 0.8904109589041096 0.9013698630136986 27\n",
      "44 0.8931506849315068 0.9013698630136986 27\n",
      "45 0.8958904109589041 0.9013698630136986 27\n",
      "46 0.8986301369863013 0.9013698630136986 27\n",
      "47 0.9013698630136986 0.9013698630136986 27\n",
      "48 0.9041095890410958 0.9041095890410958 48\n",
      "49 0.9013698630136986 0.9041095890410958 48\n",
      "50 0.9013698630136986 0.9041095890410958 48\n",
      "51 0.9041095890410958 0.9041095890410958 48\n",
      "52 0.9013698630136986 0.9041095890410958 48\n",
      "53 0.9068493150684932 0.9068493150684932 53\n",
      "54 0.9068493150684932 0.9068493150684932 53\n",
      "55 0.9068493150684932 0.9068493150684932 53\n",
      "56 0.9068493150684932 0.9068493150684932 53\n",
      "57 0.9068493150684932 0.9068493150684932 53\n",
      "58 0.9041095890410958 0.9068493150684932 53\n",
      "59 0.8986301369863013 0.9068493150684932 53\n",
      "60 0.9123287671232877 0.9123287671232877 60\n",
      "61 0.9041095890410958 0.9123287671232877 60\n",
      "62 0.9095890410958904 0.9123287671232877 60\n",
      "63 0.9095890410958904 0.9123287671232877 60\n",
      "64 0.9095890410958904 0.9123287671232877 60\n",
      "65 0.9041095890410958 0.9123287671232877 60\n",
      "66 0.9068493150684932 0.9123287671232877 60\n",
      "67 0.9068493150684932 0.9123287671232877 60\n",
      "68 0.9068493150684932 0.9123287671232877 60\n",
      "69 0.9095890410958904 0.9123287671232877 60\n",
      "70 0.9095890410958904 0.9123287671232877 60\n",
      "71 0.9123287671232877 0.9123287671232877 60\n",
      "72 0.9068493150684932 0.9123287671232877 60\n",
      "73 0.9123287671232877 0.9123287671232877 60\n",
      "74 0.9095890410958904 0.9123287671232877 60\n",
      "75 0.9123287671232877 0.9123287671232877 60\n",
      "76 0.915068493150685 0.915068493150685 76\n",
      "77 0.915068493150685 0.915068493150685 76\n",
      "78 0.915068493150685 0.915068493150685 76\n",
      "79 0.9123287671232877 0.915068493150685 76\n",
      "80 0.915068493150685 0.915068493150685 76\n",
      "81 0.915068493150685 0.915068493150685 76\n",
      "82 0.9178082191780822 0.9178082191780822 82\n",
      "83 0.9178082191780822 0.9178082191780822 82\n",
      "84 0.9178082191780822 0.9178082191780822 82\n",
      "85 0.915068493150685 0.9178082191780822 82\n",
      "86 0.915068493150685 0.9178082191780822 82\n",
      "87 0.9123287671232877 0.9178082191780822 82\n",
      "88 0.9178082191780822 0.9178082191780822 82\n",
      "89 0.915068493150685 0.9178082191780822 82\n",
      "90 0.9178082191780822 0.9178082191780822 82\n",
      "91 0.915068493150685 0.9178082191780822 82\n",
      "92 0.9178082191780822 0.9178082191780822 82\n",
      "93 0.9123287671232877 0.9178082191780822 82\n",
      "94 0.9178082191780822 0.9178082191780822 82\n",
      "95 0.9123287671232877 0.9178082191780822 82\n",
      "96 0.915068493150685 0.9178082191780822 82\n",
      "97 0.9178082191780822 0.9178082191780822 82\n",
      "98 0.9205479452054794 0.9205479452054794 98\n",
      "99 0.9205479452054794 0.9205479452054794 98\n",
      "100 0.9205479452054794 0.9205479452054794 98\n",
      "101 0.9178082191780822 0.9205479452054794 98\n",
      "102 0.9205479452054794 0.9205479452054794 98\n",
      "103 0.915068493150685 0.9205479452054794 98\n",
      "104 0.9178082191780822 0.9205479452054794 98\n",
      "105 0.9123287671232877 0.9205479452054794 98\n",
      "106 0.9178082191780822 0.9205479452054794 98\n",
      "107 0.915068493150685 0.9205479452054794 98\n",
      "108 0.9178082191780822 0.9205479452054794 98\n",
      "109 0.915068493150685 0.9205479452054794 98\n",
      "110 0.9178082191780822 0.9205479452054794 98\n",
      "111 0.915068493150685 0.9205479452054794 98\n",
      "112 0.9205479452054794 0.9205479452054794 98\n",
      "113 0.9178082191780822 0.9205479452054794 98\n",
      "114 0.9205479452054794 0.9205479452054794 98\n",
      "115 0.9205479452054794 0.9205479452054794 98\n",
      "116 0.9205479452054794 0.9205479452054794 98\n",
      "117 0.915068493150685 0.9205479452054794 98\n",
      "118 0.9178082191780822 0.9205479452054794 98\n",
      "119 0.9123287671232877 0.9205479452054794 98\n",
      "120 0.9123287671232877 0.9205479452054794 98\n",
      "121 0.9095890410958904 0.9205479452054794 98\n",
      "122 0.9178082191780822 0.9205479452054794 98\n",
      "123 0.9123287671232877 0.9205479452054794 98\n",
      "124 0.915068493150685 0.9205479452054794 98\n",
      "125 0.9123287671232877 0.9205479452054794 98\n",
      "126 0.9123287671232877 0.9205479452054794 98\n",
      "127 0.9123287671232877 0.9205479452054794 98\n",
      "128 0.915068493150685 0.9205479452054794 98\n",
      "129 0.915068493150685 0.9205479452054794 98\n",
      "130 0.915068493150685 0.9205479452054794 98\n",
      "131 0.9123287671232877 0.9205479452054794 98\n",
      "132 0.915068493150685 0.9205479452054794 98\n",
      "133 0.9123287671232877 0.9205479452054794 98\n",
      "134 0.915068493150685 0.9205479452054794 98\n",
      "135 0.9123287671232877 0.9205479452054794 98\n",
      "136 0.9123287671232877 0.9205479452054794 98\n",
      "137 0.9095890410958904 0.9205479452054794 98\n",
      "138 0.915068493150685 0.9205479452054794 98\n",
      "139 0.9068493150684932 0.9205479452054794 98\n",
      "140 0.9095890410958904 0.9205479452054794 98\n",
      "141 0.9068493150684932 0.9205479452054794 98\n",
      "142 0.9068493150684932 0.9205479452054794 98\n",
      "143 0.9041095890410958 0.9205479452054794 98\n",
      "144 0.9068493150684932 0.9205479452054794 98\n",
      "145 0.9041095890410958 0.9205479452054794 98\n",
      "146 0.9041095890410958 0.9205479452054794 98\n",
      "147 0.9041095890410958 0.9205479452054794 98\n",
      "148 0.9041095890410958 0.9205479452054794 98\n",
      "149 0.9041095890410958 0.9205479452054794 98\n",
      "150 0.9041095890410958 0.9205479452054794 98\n",
      "151 0.9041095890410958 0.9205479452054794 98\n",
      "152 0.9068493150684932 0.9205479452054794 98\n",
      "153 0.9041095890410958 0.9205479452054794 98\n",
      "154 0.9068493150684932 0.9205479452054794 98\n",
      "155 0.9068493150684932 0.9205479452054794 98\n",
      "156 0.9095890410958904 0.9205479452054794 98\n",
      "157 0.9068493150684932 0.9205479452054794 98\n",
      "158 0.9095890410958904 0.9205479452054794 98\n",
      "159 0.9068493150684932 0.9205479452054794 98\n",
      "160 0.9068493150684932 0.9205479452054794 98\n",
      "161 0.9068493150684932 0.9205479452054794 98\n",
      "162 0.9095890410958904 0.9205479452054794 98\n",
      "163 0.9095890410958904 0.9205479452054794 98\n",
      "164 0.9095890410958904 0.9205479452054794 98\n",
      "165 0.9068493150684932 0.9205479452054794 98\n",
      "166 0.9123287671232877 0.9205479452054794 98\n",
      "167 0.9068493150684932 0.9205479452054794 98\n",
      "168 0.9095890410958904 0.9205479452054794 98\n",
      "169 0.9068493150684932 0.9205479452054794 98\n",
      "170 0.9068493150684932 0.9205479452054794 98\n",
      "171 0.9068493150684932 0.9205479452054794 98\n",
      "172 0.9123287671232877 0.9205479452054794 98\n",
      "173 0.9095890410958904 0.9205479452054794 98\n",
      "174 0.9123287671232877 0.9205479452054794 98\n",
      "175 0.9123287671232877 0.9205479452054794 98\n",
      "176 0.9123287671232877 0.9205479452054794 98\n",
      "177 0.9095890410958904 0.9205479452054794 98\n",
      "178 0.9123287671232877 0.9205479452054794 98\n",
      "179 0.9095890410958904 0.9205479452054794 98\n",
      "180 0.9095890410958904 0.9205479452054794 98\n",
      "181 0.9095890410958904 0.9205479452054794 98\n",
      "182 0.9095890410958904 0.9205479452054794 98\n",
      "183 0.9068493150684932 0.9205479452054794 98\n",
      "184 0.9123287671232877 0.9205479452054794 98\n",
      "185 0.9095890410958904 0.9205479452054794 98\n",
      "186 0.9123287671232877 0.9205479452054794 98\n",
      "187 0.9095890410958904 0.9205479452054794 98\n",
      "188 0.9123287671232877 0.9205479452054794 98\n",
      "189 0.9123287671232877 0.9205479452054794 98\n",
      "190 0.9123287671232877 0.9205479452054794 98\n",
      "191 0.9095890410958904 0.9205479452054794 98\n",
      "192 0.9095890410958904 0.9205479452054794 98\n",
      "193 0.9123287671232877 0.9205479452054794 98\n"
     ]
    }
   ],
   "source": [
    "# 2.1 - RF (F1)\n",
    "#--------------\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "mx=0\n",
    "ind=0\n",
    "#3742 0.8547945205479452 0.9013698630136986 376\n",
    "\n",
    "for i in range(1, 200):\n",
    "    seed = i\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, Y, test_size=0.2, random_state=376)\n",
    "\n",
    "    rf = RandomForestClassifier(random_state=1045, n_estimators=seed)\n",
    "    rf.fit(X_train, y_train)\n",
    "    rf_pred = rf.predict(X_test)\n",
    "\n",
    "    structured_acc = accuracy_score(y_test, rf_pred)\n",
    "    \n",
    "    if structured_acc>mx:\n",
    "        mx=structured_acc\n",
    "        ind=i\n",
    "\n",
    "    print(i, structured_acc, mx, ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
